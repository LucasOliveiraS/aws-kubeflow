{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\r\n",
      "drwxrwsr-x 12 root   users  4096 Jun  2 16:17 .\r\n",
      "drwxr-xr-x  1 root   root     20 Apr  6 06:04 ..\r\n",
      "drwxrwS---  4 jovyan users  4096 Jun  2 15:48 .cache\r\n",
      "drwxrwsr-x  3 jovyan users  4096 Jun  2 15:48 .config\r\n",
      "drwxrwsr-x 15 jovyan users  4096 Jun  2 15:39 examples\r\n",
      "drwxr-sr-x  2 jovyan users  4096 Jun  2 16:18 github-issues-data\r\n",
      "drwxr-sr-x  2 jovyan users  4096 Jun  2 16:15 .ipynb_checkpoints\r\n",
      "drwxrwsr-x  5 jovyan users  4096 Jun  2 15:40 .ipython\r\n",
      "drwxrwsr-x  2 jovyan users  4096 Jun  2 15:57 .jupyter\r\n",
      "drwxrwsr-x  2 jovyan users  4096 Jun  2 15:47 .keras\r\n",
      "drwxrwS---  3 jovyan users  4096 Jun  2 15:37 .local\r\n",
      "drwxrwS---  2 root   users 16384 Jun  2 15:35 lost+found\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the github-issues-data volume is mounted in /home/jovyan\n",
    "!ls -la /home/jovyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=/home/jovyan/github-issues-data\n"
     ]
    }
   ],
   "source": [
    "# Set path for data dir\n",
    "%env DATA_DIR=/home/jovyan/github-issues-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-02 16:17:22--  https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.26.48, 2404:6800:4004:80c::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.26.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1027424178 (980M) [application/zip]\n",
      "Saving to: ‘/home/jovyan/github-issues-data/github-issues.zip’\n",
      "\n",
      "github-issues.zip   100%[===================>] 979.83M  74.5MB/s    in 14s     \n",
      "\n",
      "2019-06-02 16:17:36 (71.6 MB/s) - ‘/home/jovyan/github-issues-data/github-issues.zip’ saved [1027424178/1027424178]\n",
      "\n",
      "Archive:  /home/jovyan/github-issues-data/github-issues.zip\n",
      "  inflating: /home/jovyan/github-issues-data/github_issues.csv  \n"
     ]
    }
   ],
   "source": [
    "# Download the github-issues.zip training data to /mnt/github-issues-data\n",
    "!wget --directory-prefix=${DATA_DIR} https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
    "\n",
    "# Unzip the file into /mnt/github-issues-data directory\n",
    "!unzip ${DATA_DIR}/github-issues.zip -d ${DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a symlink from <current_directory>/github-issues-data to /mnt/github-issues-data\n",
    "!ln -sf ${DATA_DIR} github-issues-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 2.7G Jan 17  2018 github-issues-data/github_issues.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the github-issues-data symlink is created\n",
    "!ls -lh github-issues-data/github_issues.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set and preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,800 rows 3 columns\n",
      "Test: 200 rows 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1813266</th>\n",
       "      <td>\"https://github.com/fmadio/fmad20_issues/issues/182\"</td>\n",
       "      <td>igmpv2 joins inconsistent behaviour</td>\n",
       "      <td>when issuing a large number of igmp joins on the capture interfaces ~ 100 per port. somehow only some of the joins make it to the final destination and successfully subscribe the the multicast channel. first step is to capture the actual joins being sent on the capture interfaces. maybe some of them arent actually making it onto the wire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784011</th>\n",
       "      <td>\"https://github.com/beakerbrowser/beaker/issues/270\"</td>\n",
       "      <td>wrong tooltip - tab close button</td>\n",
       "      <td>operation system: ubuntu 16.10 beaker version: 0.6.0 master:latest imo an image is worth more than 1000 words : the new tab tooltip is still active when close element is already selected. ! beaker-issueg https://cloud.githubusercontent.com/assets/374655/22203279/a4699508-e16c-11e6-9cb5-95ca21333cf6.gif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394489</th>\n",
       "      <td>\"https://github.com/fnjv-feedback/fnjv-fonotecaneotropical/issues/18\"</td>\n",
       "      <td>portal usage statistics are almost back</td>\n",
       "      <td>thanks to the financial support of the museum of vertebrate zoology at berkeley, we have fixed the issues that were preventing us from logging the vertnet statistics of data use. usage statistics are being collected once again. we are now working on the reporting and visualization of those stats, so that we can bring those back to the natural history collections community in a friendly, useful modality. we expect all of this to be up and running before the end of the year. we apologize for a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     issue_url  \\\n",
       "1813266                   \"https://github.com/fmadio/fmad20_issues/issues/182\"   \n",
       "1784011                   \"https://github.com/beakerbrowser/beaker/issues/270\"   \n",
       "2394489  \"https://github.com/fnjv-feedback/fnjv-fonotecaneotropical/issues/18\"   \n",
       "\n",
       "                                     issue_title  \\\n",
       "1813266      igmpv2 joins inconsistent behaviour   \n",
       "1784011         wrong tooltip - tab close button   \n",
       "2394489  portal usage statistics are almost back   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body  \n",
       "1813266                                                                                                                                                                  when issuing a large number of igmp joins on the capture interfaces ~ 100 per port. somehow only some of the joins make it to the final destination and successfully subscribe the the multicast channel. first step is to capture the actual joins being sent on the capture interfaces. maybe some of them arent actually making it onto the wire  \n",
       "1784011                                                                                                                                                                                                      operation system: ubuntu 16.10 beaker version: 0.6.0 master:latest imo an image is worth more than 1000 words : the new tab tooltip is still active when close element is already selected. ! beaker-issueg https://cloud.githubusercontent.com/assets/374655/22203279/a4699508-e16c-11e6-9cb5-95ca21333cf6.gif  \n",
       "2394489  thanks to the financial support of the museum of vertebrate zoology at berkeley, we have fixed the issues that were preventing us from logging the vertnet statistics of data use. usage statistics are being collected once again. we are now working on the reporting and visualization of those stats, so that we can bring those back to the natural history collections community in a friendly, useful modality. we expect all of this to be up and running before the end of the year. we apologize for a...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file='github-issues-data/github_issues.csv'\n",
    "\n",
    "# read in data sample 200 rows (for speed of tutorial)\n",
    "# Set this to False to train on the entire dataset\n",
    "use_sample_data=True\n",
    "\n",
    "if use_sample_data:\n",
    "    training_data_size=200\n",
    "    traindf, testdf = train_test_split(pd.read_csv(data_file).sample(n=training_data_size), \n",
    "                                   test_size=.10)\n",
    "else:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(data_file),test_size=.10)\n",
    "\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to lists in preparation for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "#preview output of first element\n",
    "train_body_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data For Deep Learning\n",
    "\n",
    "See [this repo](https://github.com/hamelsmu/ktext) for documentation on the ktext package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 1 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 1,800 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " first, this is very useful, thanks for creating! i switched my emacs configuration to use dante ; because i do not want to depend either on stack/intero or ghc-mod . however, dante still depends on cabal-install and which i do not generally install because i use nix instead. is there a way to use only only ghci instead of cabal-repl ? i'm ready to give up some more features if they require cabal repl . but still depending on cabal-install is better than ghc-mod or intero ; since it's easier to install and much more common. \n",
      "\n",
      "after pre-processing:\n",
      " [ 104   13    8  302  512  110   14  690    6 3305   50 6245  303    4\n",
      "   42 4254  141    6   44   20  123    4 2297  480   16  428 2044   24\n",
      " 1118  714  212 4254  205 1508   16 2709  150    7   62    6   44   20\n",
      " 1845  150  141    6   42 2045  163    8   40    5  165    4   42   73\n",
      "   73 6246  163   12 2709 2046    6   59 1846    4  745   88   66  105] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 0 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 1,800 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a text processor for the titles, with some different parameters\n",
    "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
    "#                      document\n",
    "#  padding = 'post' means that zero padding is appended to the end of the \n",
    "#             of the document (as opposed to the default which is 'pre')\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " possibility to use without cabal-install .\n",
      "after pre-processing:\n",
      " [   2  832    5   23  158 1406   87    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize all of this to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs.npy', train_title_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from disk into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1800, 70)\n",
      "Shape of decoder input: (1800, 11)\n",
      "Shape of decoder target: (1800, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp.dpkl: 8002\n",
      "Size of vocabulary for title_pp.dpkl: 4158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1247400     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4158)   1251558     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 5,984,958\n",
      "Trainable params: 5,983,158\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 547.00 410.00\" width=\"547pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 543,-406 543,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140534416436640 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140534416436640</title>\n",
       "<polygon fill=\"none\" points=\"59,-365.5 59,-401.5 268,-401.5 268,-365.5 59,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163.5\" y=\"-379.8\">Decoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140534416734248 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140534416734248</title>\n",
       "<polygon fill=\"none\" points=\"15.5,-292.5 15.5,-328.5 311.5,-328.5 311.5,-292.5 15.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163.5\" y=\"-306.8\">Decoder-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140534416436640&#45;&gt;140534416734248 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140534416436640-&gt;140534416734248</title>\n",
       "<path d=\"M163.5,-365.4551C163.5,-357.3828 163.5,-347.6764 163.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"167.0001,-338.5903 163.5,-328.5904 160.0001,-338.5904 167.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140534323348088 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140534323348088</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 327,-255.5 327,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163.5\" y=\"-233.8\">Decoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140534416734248&#45;&gt;140534323348088 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140534416734248-&gt;140534323348088</title>\n",
       "<path d=\"M163.5,-292.4551C163.5,-284.3828 163.5,-274.6764 163.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"167.0001,-265.5903 163.5,-255.5904 160.0001,-265.5904 167.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140534325281400 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140534325281400</title>\n",
       "<polygon fill=\"none\" points=\"330,-292.5 330,-328.5 539,-328.5 539,-292.5 330,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434.5\" y=\"-306.8\">Encoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140534325282632 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140534325282632</title>\n",
       "<polygon fill=\"none\" points=\"345,-219.5 345,-255.5 524,-255.5 524,-219.5 345,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434.5\" y=\"-233.8\">Encoder-Model: Model</text>\n",
       "</g>\n",
       "<!-- 140534325281400&#45;&gt;140534325282632 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140534325281400-&gt;140534325282632</title>\n",
       "<path d=\"M434.5,-292.4551C434.5,-284.3828 434.5,-274.6764 434.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"438.0001,-265.5903 434.5,-255.5904 431.0001,-265.5904 438.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140534323350832 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140534323350832</title>\n",
       "<polygon fill=\"none\" points=\"220,-146.5 220,-182.5 377,-182.5 377,-146.5 220,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-160.8\">Decoder-GRU: GRU</text>\n",
       "</g>\n",
       "<!-- 140534323348088&#45;&gt;140534323350832 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140534323348088-&gt;140534323350832</title>\n",
       "<path d=\"M196.8708,-219.4551C214.8007,-209.7596 237.0905,-197.7066 256.2227,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"257.9137,-190.4257 265.0452,-182.5904 254.5841,-184.2682 257.9137,-190.4257\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140534325282632&#45;&gt;140534323350832 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140534325282632-&gt;140534323350832</title>\n",
       "<path d=\"M400.882,-219.4551C382.8193,-209.7596 360.3644,-197.7066 341.0904,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"342.6689,-184.236 332.2026,-182.5904 339.3582,-190.4037 342.6689,-184.236\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140534318909368 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140534318909368</title>\n",
       "<polygon fill=\"none\" points=\"135,-73.5 135,-109.5 462,-109.5 462,-73.5 135,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-87.8\">Decoder-Batchnorm-2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140534323350832&#45;&gt;140534318909368 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140534323350832-&gt;140534318909368</title>\n",
       "<path d=\"M298.5,-146.4551C298.5,-138.3828 298.5,-128.6764 298.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"302.0001,-119.5903 298.5,-109.5904 295.0001,-119.5904 302.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140534318796472 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140534318796472</title>\n",
       "<polygon fill=\"none\" points=\"193.5,-.5 193.5,-36.5 403.5,-36.5 403.5,-.5 193.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-14.8\">Final-Output-Dense: Dense</text>\n",
       "</g>\n",
       "<!-- 140534318909368&#45;&gt;140534318796472 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140534318909368-&gt;140534318796472</title>\n",
       "<path d=\"M298.5,-73.4551C298.5,-65.3828 298.5,-55.6764 298.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"302.0001,-46.5903 298.5,-36.5904 295.0001,-46.5904 302.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1584 samples, validate on 216 samples\n",
      "Epoch 1/7\n",
      "1584/1584 [==============================] - 13s 8ms/step - loss: 8.3049 - val_loss: 7.1726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "1584/1584 [==============================] - 11s 7ms/step - loss: 7.0294 - val_loss: 6.3375\n",
      "Epoch 3/7\n",
      "1584/1584 [==============================] - 11s 7ms/step - loss: 6.2218 - val_loss: 5.8373\n",
      "Epoch 4/7\n",
      "1584/1584 [==============================] - 11s 7ms/step - loss: 5.7035 - val_loss: 5.4728\n",
      "Epoch 5/7\n",
      "1584/1584 [==============================] - 11s 7ms/step - loss: 5.3119 - val_loss: 5.2600\n",
      "Epoch 6/7\n",
      "1584/1584 [==============================] - 11s 7ms/step - loss: 4.9847 - val_loss: 5.0687\n",
      "Epoch 7/7\n",
      "1584/1584 [==============================] - 11s 7ms/step - loss: 4.7163 - val_loss: 4.9491\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Example Results On Holdout Set\n",
    "\n",
    "It is useful to see examples of real predictions on a holdout set to get a sense of the performance of the model.  We will also evaluate the model numerically in a following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 97 =================\n",
      "\n",
      "\"https://github.com/FirefoxUX/photon/issues/170\"\n",
      "Issue Body:\n",
      " define document, modify, finalise assets, finalise copy, extra checks. \n",
      "\n",
      "Original Title:\n",
      " radio buttons components: define\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " expectation try rgb linking titlers zenhub quote tuning robo binding coral been\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 195 =================\n",
      "\n",
      "\"https://github.com/faceyspacey/redux-first-router/issues/90\"\n",
      "Issue Body:\n",
      " the onbeforechange function is not promise aware. this makes it very hard to involve any async operation to decide whether we cancel the route through dispatching a redirect or not. would be great to have a onbeforechange function which will honor: 1. a promise dispatched from within onbeforechange like redux-thunk 2. or will await a promise returned from onbeforechange and will evaluate the skipping after the promise resolved. this will enable a lot of possible use-cases like authentication an existing cookie/jwt does not guarantee that it is still valid , near real-time stock/reservation checks, lock/conflict handling etc. looking into the code it seems feasible without a lot of change. if this is something you would consider as a pr i would start working on it. \n",
      "\n",
      "Original Title:\n",
      " make onbeforechange honor promises\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail enjoyable audible ubuntu enforce ans ok defining floorplan triggered area\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 65 =================\n",
      "\n",
      "\"https://github.com/gxc346/prj-rev-bwfs-dasmoto/issues/3\"\n",
      "Issue Body:\n",
      " you've got all your code sections inside container divs, which is great for readability and makes layout/styling easier! this is a good practice so keep it up! https://github.com/gxc346/prj-rev-bwfs-dasmoto/blob/master/dasmoto's%20arts%20%26%20crafts/index.html l13-l33 \n",
      "\n",
      "Original Title:\n",
      " good work wrapping sections in container divs\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " super rdf chromedriver 초기화면에 me genomes detach sum referering hooks triggered area\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 192 =================\n",
      "\n",
      "\"https://github.com/JoshuaSkelly/delver-example-mods/issues/23\"\n",
      "Issue Body:\n",
      " i remember this bug happened before but i thought you fixed it in a stream :o \n",
      "\n",
      "Original Title:\n",
      " equipping item from slot 1 makes the item you swap with disappear\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " purpose cluck anecdotal datasets mejorar stronghold library placeholder uniform contributing mcxtrace there\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 70 =================\n",
      "\n",
      "\"https://github.com/likewater/dada-poetry/issues/5\"\n",
      "Issue Body:\n",
      " minimum: jumbotron, intro area 12 wide, text input area, text output area. remember: container, rows, columns, panels, buttons. code what we need to show a mvp. \n",
      "\n",
      "Original Title:\n",
      " 1. mvp - front end: create the html skeleton - no content\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " pt loaded must contact contact recreatewnd buttons album opengl marks interface continuating\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 18 =================\n",
      "\n",
      "\"https://github.com/OrchardCMS/Orchard2/issues/811\"\n",
      "Issue Body:\n",
      " i use _session.query<user,userindex> user=>emailarrys.contains user.normalizedusername can't find user,must use _session.query<user, userindex> .where user=>emailarrys.contains user.normalizedusername \n",
      "\n",
      "Original Title:\n",
      " can't find user use query<t,tindex> with linq\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " selective compiling tar mangling limpian simple mmc listen replies preference loadfromassemblypath tried\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 149 =================\n",
      "\n",
      "\"https://github.com/shuhongwu/hockeyapp/issues/27287\"\n",
      "Issue Body:\n",
      " version: 7.1.0 3117 | com.sina.weibo reason no reason found. full stack trace includes javascriptcore, webcore, corefoundation, cfnetwork, libdispatch.dylib, libsystem_pthread.dylib. link to hockeyapp https://rink.hockeyapp.net/manage/apps/411124/crash_reasons/165514486 https://rink.hockeyapp.net/manage/apps/411124/crash_reasons/165514486 \n",
      "\n",
      "Original Title:\n",
      " fix crash in jsc::jslock::dropalllocks::dropalllocks jsc::vm&\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " unstyled summit centre datasets local delay activated thirdparty map abnormal datasets protocol\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 48 =================\n",
      "\n",
      "\"https://github.com/tmk/tmk_keyboard/issues/502\"\n",
      "Issue Body:\n",
      " my lgui key stopped working for no reason but the hid listener get's something from the keyboard, here's the output: r/c 0123456789abcdef 00: 0000000000000000 01: 0000000000000000 02: 0000000000000000 03: 0000000000000000 04: 0100000000000000 r/c 0123456789abcdef 00: 0000000000000000 01: 0000000000000000 02: 0000000000000000 03: 0000000000000000 04: 0000000000000000 when i change the key to another keycode it works \n",
      "\n",
      "Original Title:\n",
      " gui key not working\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " thumbnail section swiped inactive center participate diff man experience experience revert wire\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 122 =================\n",
      "\n",
      "\"https://github.com/CenturyLinkCloud/mdw/issues/74\"\n",
      "Issue Body:\n",
      " run process named fallouttasktest open the instance and proceed on open task a4 from designer. after the process is complete task a4 is still in waiting status and and shown in yellow but in mdw hub view it shows in black. i think task should be shown in cancelled color in both places. example url in mdw hub http://localhost:8080/mdw/ /workflow/processes/10216 \n",
      "\n",
      "Original Title:\n",
      " mdw hub process ui view does not show correct color on waiting activity of a completed process\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " purpose верна nest plexpy recompile polygon charaset host paper paper push action\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 128 =================\n",
      "\n",
      "\"https://github.com/souporserious/react-popper/issues/15\"\n",
      "Issue Body:\n",
      " i just noticed that if i use the following code: js <popper placement= top > {this.state.isopen && <div>test</div> } </popper> there are issues with the placement of the popper content. i'm guessing this is because the popper is being measured when it has no contents. in the simple case this is easy to fix by moving the conditional outside of the popper. that being said, i did some more digging and found that if the contents of the popper component change its size/position are not currently recalculated. it would be nice if the popper component would handle updates to its contents. \n",
      "\n",
      "Original Title:\n",
      " recalculate size of popper when its contents change?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " kubelet credentials fp 팝업 interface las mayoreo uart mangling selected alice sourcemap\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5 =================\n",
      "\n",
      "\"https://github.com/GoogleCloudPlatform/nodejs-docs-samples/issues/348\"\n",
      "Issue Body:\n",
      " daharon-macpro:speech daharon$ node recognize.js sync ./resources/audio.raw -e linear16 -r 16000 transcription: how old is the brooklyn bridge daharon-macpro:speech daharon$ node recognize.js async ./resources/audio.raw -e linear16 -r 16000 transcription: how old is the brooklyn bridge,, daharon-macpro:speech daharon$ node recognize.js async ./resources/audio.raw -e linear16 -r 16000 transcription: how old is the brooklyn bridge,, \n",
      "\n",
      "Original Title:\n",
      " async seems to create an additional ,, added to the transcript\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " broadcast sapeurs skal loading pages measure passionately passionately c department wording обратного\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 27 =================\n",
      "\n",
      "\"https://github.com/profeIAP/pruebas/issues/40\"\n",
      "Issue Body:\n",
      " usando pitivi, editar el vídeo de la tarea 95 de modo que: 1. se eliminen todas las partes que sobran como p.e. el final en el que se ve cómo su autor para recordmydesktop 1. se incluya la narración asociada a dicha tarea.\n",
      "1. se elimine el sonido de fondo que incluye el vídeo original para evitar que se solape con la narración . de especial utilidad te resultará el vídeo generado en la tarea 52 \n",
      "\n",
      "Original Title:\n",
      " editar vídeo cómo iniciar modo navegación privada por defecto en chromium\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " behavior selective compiling metapackage link just off librepgp temperature hook filenotfoundexception selected\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 22 =================\n",
      "\n",
      "\"https://github.com/anqi-lu/TMDB-keywords/issues/11\"\n",
      "Issue Body:\n",
      " - make sure every interaction works as expected. - adjust font, font size, colors to ensure readability. - adjust color for to help users understanding. \n",
      "\n",
      "Original Title:\n",
      " debug and fine tuning\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " kubelet jmod october kong adjust temporarily canonical height tumor single egregious lineartransformers\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 90 =================\n",
      "\n",
      "\"https://github.com/minetest/minetest/issues/5854\"\n",
      "Issue Body:\n",
      " missing lua api documentation for the upcoming 0.4.16 release, i have reviewed lua_api.txt today. these are the things which i think are currently lacking in documention: nodes - the various drawtypes are almost entirely undefined - type= leveled for nodeboxes: it is unclear what happens to the y values in the boxes. are they ignored? start values? or something special? formspecs - anchor has no definition of what it does. doc just says you can set it to some position which is not helpful - syntax/semantics of the fields table you get from on_receive_fields is largely undocumented. there are many gotchas like the undocumented quit field - it is poorly documented which events trigger an on_receive_fields and which fields are sent then. one gotcha is that checkboxes are sent only when they themselves are clicked, but not when you clicked a button in the same formular classes - concept of classes is undefined. - it is not mentinoned that you have to do e.g. x = itemstack to create a new object. - it is not mentioned that you have to do foo:bar on these objects - metadataref is not explained. - if this is considered an abstract class, say that. other - scope of minetest.register_alias is not mentioned. what is aliased? entitites? items? nodes? players? - vector functions are mostly undefined - semantic/syntax of itemstrings is undefined <id> <itemcount> <wear> - list of special groups does not mention punch_operable used by entities --- note: by “undefined”, i mean, no formal definition is written down. examples alone do not count! examples can only supplement, but not replace a formal definition. \n",
      "\n",
      "Original Title:\n",
      " missing lua api documentation\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " purpose cluck anecdotal caches ubuntu storing showing day normal sideeffects misspelled pin\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 132 =================\n",
      "\n",
      "\"https://github.com/facebook-csharp-sdk/facebook-csharp-sdk/issues/371\"\n",
      "Issue Body:\n",
      " hi everybody. i've set a challenge for my self to retrieve all users feed posts in once but i don't know how to do cursor based pagination with facebook c sdk. is there any workaround. here is what i have tried. public async task<actionresult> fb_getfeed { var access_token = httpcontext.items access_token .tostring ; if !string.isnullorempty access_token { var appsecret_proof = access_token.generateappsecretproof ; var fb = new facebookclient access_token ; dynamic myfeed = await fb.gettaskasync me/feed?fields=id,from {{id, name, picture{{url}} }},story,picture,link,name,description, + message,type,created_time,likes,comments .graphapicall appsecret_proof ; var postlist = new list<facebookpostviewmodel> ; string nextpageuri = string.empty; if myfeed.paging != null && myfeed.paging.next != null nextpageuri = myfeed.paging.next; foreach dynamic post in myfeed.data { postlist.add dynamicextension.tostatic<facebookpostviewmodel> post ; } return partialview postlist ; } else throw new httpexception 404, missing access token ; } \n",
      "\n",
      "Original Title:\n",
      " how to do cursor based pagination with facebook c sdk?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " alloy behavior 3d schwag disabled pr before tinker slow hirundo authorizations exhibits\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 62 =================\n",
      "\n",
      "\"https://github.com/themexpert/digicom/issues/383\"\n",
      "Issue Body:\n",
      " steps to reproduce the issue if i use a price per year and the price is over 1000 euro then the digicom will add the product with price 1€. that is a serious bug easily reproduced. i will have to change my prices to 6 months plans to work. expected result actual result system information as much as possible additional comments \n",
      "\n",
      "Original Title:\n",
      " digicom fail to manage a subscription with more than 3 digits price\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail audible define tileset injection favourites favourites setup across 4 superior\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 98 =================\n",
      "\n",
      "\"https://github.com/interaction-lab/openface_ros/issues/3\"\n",
      "Issue Body:\n",
      " thank you for sharing your work. i tried to use this package, but i cannot compile it. i show you compilation errors. do you know how to solve this problem? errors << openface_ros:check /home/ayuguchi/catkin_ws/logs/openface_ros/build.check.002.log cmake error at /home/ayuguchi/catkin_ws/src/openface_ros/cmakelists.txt:26 message : message called with incorrect number of arguments cmake error at /home/ayuguchi/catkin_ws/src/openface_ros/cmakelists.txt:27 get_filename_component : get_filename_component called with incorrect number of arguments cmake error at /home/ayuguchi/catkin_ws/src/openface_ros/cmakelists.txt:28 message : message called with incorrect number of arguments /usr/bin/cc cd /home/ayuguchi/catkin_ws/build/openface_ros; catkin build --get-env openface_ros | catkin env -si /usr/bin/make cmake_check_build_system; cd - fyi, my pc's environments are ubuntu 14.04, ros indigo, opencv 3.2, and dlib 19.4. \n",
      "\n",
      "Original Title:\n",
      " cannot compile this package\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail enjoyable lastest recommendations por matrix readers below attacks date nulio\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 194 =================\n",
      "\n",
      "\"https://github.com/lstjsuperman/fabric/issues/2547\"\n",
      "Issue Body:\n",
      " in com.immomo.momo.mk.service.mkprepareservice.onpreparefinished number of crashes: 1 impacted devices: 1 there's a lot more information about this crash on crashlytics.com: https://fabric.io/momo6/android/apps/com.immomo.momo/issues/5970636dbe077a4dccd7144f?utm_medium=service_hooks-github&utm_source=issue_impact https://fabric.io/momo6/android/apps/com.immomo.momo/issues/5970636dbe077a4dccd7144f?utm_medium=service_hooks-github&utm_source=issue_impact \n",
      "\n",
      "Original Title:\n",
      " mkprepareservice.java line 64\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " unstyled summit centre datasets local delay activated thirdparty map abnormal datasets protocol\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 17 =================\n",
      "\n",
      "\"https://github.com/faheel/Algos/issues/122\"\n",
      "Issue Body:\n",
      " i would like to work on 8 puzzle problem \n",
      "\n",
      "Original Title:\n",
      " add bfs, dfs using stack and recursion and a algorithm for 8puzzle problem\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail signatures cursor all zzbck locally x recreatewnd alphachest weechat under\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 168 =================\n",
      "\n",
      "\"https://github.com/ctfs/write-ups-2017/issues/2219\"\n",
      "Issue Body:\n",
      " http://ift.tt/2exvwjj<br><p> click on title to read </p>\n",
      "<p>url: <a href= http://ift.tt/2exvwjj >http://ift.tt/2exvwjj</a></p>\n",
      "<p>topics: web,&nbsp;ssti,&nbsp;lfi&nbsp;</p>\n",
      "rating: <span id= user_rating class= category-value >0</span>\n",
      "<p><a href= http://ift.tt/2jgqs1d >original writeup</a></p>\n",
      "<br> \n",
      "\n",
      "Original Title:\n",
      " asis ctf finals 2017 golem is stupid! team teamrocketist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck closed src autograded subdirectory django lektor charaset compatible usddefault usddefault feeds\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 173 =================\n",
      "\n",
      "\"https://github.com/intel/intel-cmt-cat/issues/61\"\n",
      "Issue Body:\n",
      " hi once again! with reference to 57, i've updated to a kernel that allows me to use the per-thread monitoring capability more specifically, 4.13 . while using the api, i.e. _ ./monitor_app -i _ also reports this i get the following warning: > warn: as of kernel 4.10, intel r rdt perf results per core are found to be incorrect. could i get any information as to what results are incorrect? should i not take any measurements obtained with the api for granted? also, does this mean per-thread results are incorrect as well? regards, toni \n",
      "\n",
      "Original Title:\n",
      " thread-level monitoring 2\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " terrain spinner switch предпросмотр cheryl accordion web ts nomethoderror wrong createdataadapter keep\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 101 =================\n",
      "\n",
      "\"https://github.com/fable-compiler/Fable/issues/731\"\n",
      "Issue Body:\n",
      " hi, it would useful for beginners to have a full html documentation hosted on the fable.io website. being a beginner, i am currently learning by doing. but by not knowing the functions available in fable and libraries such a fable-arch and their types signatures i have to search the code which is time consuming. ideally, for fable and related libraries it would be cool if we could have : - the full api of fable modules, functions and types on the website up to date with the latest release. - type annotation in code sample i think having an auto-updated documentation should be quite easy using a ci hook. so the harder part is how to generate the documentation itself. i tried for the whole day to build a prototype using fsharp.formatting https://tpetricek.github.io/fsharp.formatting/ which should solve the two points, but beside having the library running without error i did not manage to get a single web page except an empty one. \n",
      "\n",
      "Original Title:\n",
      " html documentation of fable api\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " unstyled overdrive thundercharge pb evaluated optional mangling circular circular cyan published yum\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 92 =================\n",
      "\n",
      "\"https://github.com/mixpanel/mixpanel-android/issues/507\"\n",
      "Issue Body:\n",
      " i am getting many crashes on android version 4.1.2 80% and 4.1.1 20% . i am using the following version of mixpanel library. com.mixpanel.android:mixpanel-android:5.+ here is the logcat for the crash: fatal exception: java.lang.nullpointerexception at android.util.pair.hashcode pair.java:62 at java.util.hashmap.put hashmap.java:390 at java.util.hashset.add hashset.java:95 at com.mixpanel.android.viewcrawler.viewcrawler$viewcrawlerhandler.loadeventbindings viewcrawler.java:443 at com.mixpanel.android.viewcrawler.viewcrawler$viewcrawlerhandler.loadknownchanges viewcrawler.java:382 at com.mixpanel.android.viewcrawler.viewcrawler$viewcrawlerhandler.handlemessage viewcrawler.java:321 at android.os.handler.dispatchmessage handler.java:99 at android.os.looper.loop looper.java:137 at android.os.handlerthread.run handlerthread.java:60 url please help me with the solution asap. \n",
      "\n",
      "Original Title:\n",
      " view crawler crashes\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck ci collectors ubuntu enforce ans ok pass sst pass expected ts3\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 69 =================\n",
      "\n",
      "\"https://github.com/terraform-providers/terraform-provider-cloudstack/issues/15\"\n",
      "Issue Body:\n",
      " it would be nice to have a cloudstack_template datasource that would return the latest template matching a pattern. for example: hcl datasource cloudstack_template ubuntu { most_recent = true filter { name = name value = ubuntu 16.04 . } } \n",
      "\n",
      "Original Title:\n",
      " add a cloudstack_template datasource\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail signatures story enh triggered versus webdrivers issues звонка inform findpoas\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 95 =================\n",
      "\n",
      "\"https://github.com/tunnckoCore/ideas/issues/82\"\n",
      "Issue Body:\n",
      " https://prepack.io/getting-started.html - rollup-plugin-prepack https://npm.im/rollup-plugin-prepack - jstransformer-prepack https://npm.im/jstransformer-prepack \n",
      "\n",
      "Original Title:\n",
      " rollup and jstransformer for prepack\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " don unnecessary requests summary stay algernon delay activated thirdparty stampede lift invalidate\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 54 =================\n",
      "\n",
      "\"https://github.com/UNC-Libraries/MARC-record-set-wrangler/issues/7\"\n",
      "Issue Body:\n",
      " this error message is given, even if the dupe records are in the existing record set. script failure! duplicate records in incoming record file s : multiple records in your incoming record file s have the same 001 value s . affected 001 values: 611632883 please duplicate your incoming file s and try the script again. be specific about which file set has dupes! \n",
      "\n",
      "Original Title:\n",
      " misleading error message re: dupe records in file\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " expectation rustdoc loaded must making chained businesses pipe ht murderer color yum\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 116 =================\n",
      "\n",
      "\"https://github.com/hosley/checkin/issues/46\"\n",
      "Issue Body:\n",
      " this is because the classroom data object is passed to the same mapvalues pipe that all the other lists were, and in said pipe i remove the rooms with keys < 0. i could fix this by: - making a distinct map values pipe for the classroom provider - instead of using a pipe, array.from this.classroomservice.data.values might work but this makes me feel conflicted since it would somewhat make the mapvalues pipe look silly - the unallocated room could have some other _id, namely 0 because thats the only room number that i can see them not using and is still a room number, i could make the _id's not numbers but that might also require refactoring elsewhere \n",
      "\n",
      "Original Title:\n",
      " unallocated room not showing up in the admin classroom tab\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " super thumbnail audible homebrew gem supports 2k implementations clarity library others son\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 148 =================\n",
      "\n",
      "\"https://github.com/nicolaskruchten/jupyter_pivottablejs/issues/19\"\n",
      "Issue Body:\n",
      " hiya nicolas, i'm finding that jupyter launches pivots on my laptop just fine, but am having issues at work. on the latter, i'm behind a firewall and also running on a vmware instance in stand-alone/non-bridged mode. the proxy settings seem to be fine otherwise, and i even set http_proxy and https_proxy environment vars for anaconda to pickup automatically as well as setting ssl_verify to false. the pandas dataframes load just fine, but i'm not seeing anything in the cell where i try to display a pivot_ui using even the simple examples that you've provided. i've even started jupyter with --debug and looked at the logs, but don't see what's going on yet. any suggestions for how to debug this? much obliged for your indulgence on this, and all your fine efforts. rick p.s. saw your note elsewhere on the output path issues. when i set the output file path to an absolute location e.g. /blah/bleh/test.html i did get a 404, but when i just specified a relative path test.html it failed silently again but wrote test.html . also, i do not see the pop out text within the output cell.... it's perfectly blank.... p.p.s. i'm also trying to get chrome installed on redhat to use the javascipt debugging extensions to better isolate the issue, but haven't been successful there either yet. \n",
      "\n",
      "Original Title:\n",
      " not working in jupyter within vmware vm\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck ci collectors ubuntu enforce ans ok pass sst pass expected ts3\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 56 =================\n",
      "\n",
      "\"https://github.com/AaronParsons/aipy/issues/15\"\n",
      "Issue Body:\n",
      " recreate by running a pull_antpols with ~50 or so antennas import aipy as a uvi = a.miriad.uv 'zen.2457471.18781.xx.uvc' a.scripting.uv_selector uvi, ants=' 1,3,4,13,15,16,23,26,37,38,41,42,46,47,49,50,56,57,58,59,61,63,66,67,70,71,73,74,82,83,87,90,98,99,103,106,114,115,116,117,118,119,120,121,122,123,124,125,126,127 _ 1,3,4,13,15,16,23,26,37,38,41,42,46,47,49,50,56,57,58,59,61,63,66,67,70,71,73,74,82,83,87,90,98,99,103,106,114,115,116,117,118,119,120,121,122,123,124,125,126,127 ' zen.2457471.18781.xx.uvc is 3.7gb running this uv_selector uses about 20gb of ram. the origin seems to be that each miriad.select does a malloc for each baseline that is order maxant^2. maxant was recently increased to 2048, making this issue more significant. we are bypassing this issue for now in pull_antpols.py by selecting baselines in the uv.pipe with an mfunc \n",
      "\n",
      "Original Title:\n",
      " uv.select uses huge amounts of ram order 5x file size\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " super rdf chromedriver 초기화면에 me genomes detach sum referering hooks triggered area\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 129 =================\n",
      "\n",
      "\"https://github.com/EdwinTh/padr/issues/27\"\n",
      "Issue Body:\n",
      " thicken can still be slowish when the number of rows goes above ~million rows. look to rcppparallel for a solution. \n",
      "\n",
      "Original Title:\n",
      " improve thicken performance for very large sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail enjoyable lastest manuals spam eraikinak review parsing total pivot inicio\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 106 =================\n",
      "\n",
      "\"https://github.com/KyberNetwork/smart-contracts/issues/10\"\n",
      "Issue Body:\n",
      " during contract upgrades change only the pointer in the dummy contract. this is in addition or instead of ens domain, \n",
      "\n",
      "Original Title:\n",
      " store kybernetwork contract latest address in a dummy contract\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck closed src autograded subdirectory parser collapsible plexpy allows helm wc clone\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 82 =================\n",
      "\n",
      "\"https://github.com/betaflight/betaflight-tx-lua-scripts/issues/15\"\n",
      "Issue Body:\n",
      " i have the bf lua script installed on x7. i can read the values when i power on my quad. but i can't scroll up and down with the wheel . it's like the wheel is not working in the script page. i have last open tx nb and last bf lua script. thanks for your help \n",
      "\n",
      "Original Title:\n",
      " impossible to scroll through the page on x7\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail enjoyable with smithii unlimited codec tmuxifier both charaset wall rsa\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 125 =================\n",
      "\n",
      "\"https://github.com/James9074/lambda/issues/3\"\n",
      "Issue Body:\n",
      " right now, you have to manually copy the code into a new lambda, and you can't edit them at all. not sure what the best way to handle editing is, but i'm thinking versioning and a very easy way to flip through previous versions. \n",
      "\n",
      "Original Title:\n",
      " edit or copy lambdas\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " loaded must making chained businesses pipe ht murderer color course exit volume\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 16 =================\n",
      "\n",
      "\"https://github.com/watson-developer-cloud/node-sdk/issues/462\"\n",
      "Issue Body:\n",
      " there is a documentation bug for nodejs example add a document https://www.ibm.com/watson/developercloud/discovery/api/v1/?node add-doc . it has nested parenthesis: javascript discovery.adddocument '{environment_id}', '{collection_id}', file , function error, data { console.log json.stringify data, null, 2 ; } ; it should be a nested json object: javascript discovery.adddocument { environment_id:environment_id, collection_id:collection_id, file:file }, function error, data { console.log json.stringify data, null, 2 ; } ; \n",
      "\n",
      "Original Title:\n",
      " discovery documentation nodejs\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail enjoyable lastest manuals spam eraikinak review parsing total pivot inicio\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 178 =================\n",
      "\n",
      "\"https://github.com/intersystems-ru/webterminal/issues/85\"\n",
      "Issue Body:\n",
      " this statement does not highlights properly in webterminal: while a > 2 { write 1 write 2 } \n",
      "\n",
      "Original Title:\n",
      " syntax highlight in while loop\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " purpose верна nest plexpy recompile polygon charaset host paper paper push action\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 113 =================\n",
      "\n",
      "\"https://github.com/MyRobotLab/myrobotlab/issues/133\"\n",
      "Issue Body:\n",
      " with arduino nano, i can't read analog pin a6 and a7. i would like used a4 and a5, but it is used for i2c bus. \n",
      "\n",
      "Original Title:\n",
      " analog pin a6 and a7\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck closed src autograded subdirectory parser collapsible plexpy allows helm wc clone\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 14 =================\n",
      "\n",
      "\"https://github.com/peterhellberg/emojilib/issues/1\"\n",
      "Issue Body:\n",
      " this is super cool : i really want to try to get this working with ebiten. https://github.com/hajimehoshi/ebiten can you check this error ? cd emojilib/ x-macbook-pro:emojilib apple$ go generate x-macbook-pro:emojilib apple$ ls license readme.md _generator emojilib.go emojilib_test.go\tgenerated.go x-macbook-pro:emojilib apple$ go test --- fail: testfind 0.00s emojilib_test.go:19: unexpected error emojilib_test.go:23: find bee = , nil, want 🐝 , nil fail exit status 1 fail\tgithub.com/peterhellberg/emojilib\t0.011s \n",
      "\n",
      "Original Title:\n",
      " bee test fails\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " super thumbnail linux custom scheme regex pooleddataarray l3 poster screentime drawer aggiungere\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 113 =================\n",
      "\n",
      "\"https://github.com/MyRobotLab/myrobotlab/issues/133\"\n",
      "Issue Body:\n",
      " with arduino nano, i can't read analog pin a6 and a7. i would like used a4 and a5, but it is used for i2c bus. \n",
      "\n",
      "Original Title:\n",
      " analog pin a6 and a7\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck closed src autograded subdirectory parser collapsible plexpy allows helm wc clone\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 114 =================\n",
      "\n",
      "\"https://github.com/replicant1/TrafficAtNSW/issues/34\"\n",
      "Issue Body:\n",
      " enables different behaviour in production no log_tag necessary \n",
      "\n",
      "Original Title:\n",
      " switch to timber for logging\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " loaded must making chained businesses pipe ht murderer color course exit volume\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 61 =================\n",
      "\n",
      "\"https://github.com/eppz/VSCode.Extension.eppz_Code/issues/5\"\n",
      "Issue Body:\n",
      " i really want to add this to vs 2017! \n",
      "\n",
      "Original Title:\n",
      " can u make it compatible with visual studio 2017?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " behavior 3d 3d droid 3 falls votekicked jenkins optionally uint doing op\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 48 =================\n",
      "\n",
      "\"https://github.com/tmk/tmk_keyboard/issues/502\"\n",
      "Issue Body:\n",
      " my lgui key stopped working for no reason but the hid listener get's something from the keyboard, here's the output: r/c 0123456789abcdef 00: 0000000000000000 01: 0000000000000000 02: 0000000000000000 03: 0000000000000000 04: 0100000000000000 r/c 0123456789abcdef 00: 0000000000000000 01: 0000000000000000 02: 0000000000000000 03: 0000000000000000 04: 0000000000000000 when i change the key to another keycode it works \n",
      "\n",
      "Original Title:\n",
      " gui key not working\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " thumbnail section swiped inactive center participate diff man experience experience revert wire\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 42 =================\n",
      "\n",
      "\"https://github.com/koorellasuresh/UKRegionTest/issues/31620\"\n",
      "Issue Body:\n",
      " first from flow in uk south \n",
      "\n",
      "Original Title:\n",
      " first from flow in uk south\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " ubuntu enforce days logins split autograded buff autoboot autoboot many ak fork\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 173 =================\n",
      "\n",
      "\"https://github.com/intel/intel-cmt-cat/issues/61\"\n",
      "Issue Body:\n",
      " hi once again! with reference to 57, i've updated to a kernel that allows me to use the per-thread monitoring capability more specifically, 4.13 . while using the api, i.e. _ ./monitor_app -i _ also reports this i get the following warning: > warn: as of kernel 4.10, intel r rdt perf results per core are found to be incorrect. could i get any information as to what results are incorrect? should i not take any measurements obtained with the api for granted? also, does this mean per-thread results are incorrect as well? regards, toni \n",
      "\n",
      "Original Title:\n",
      " thread-level monitoring 2\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " terrain spinner switch предпросмотр cheryl accordion web ts nomethoderror wrong createdataadapter keep\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 177 =================\n",
      "\n",
      "\"https://github.com/yhelen-stuy/softdev-project-1/issues/1\"\n",
      "Issue Body:\n",
      " or a template file to guide me ...bc as a user i could be confused about how to include my keys in the file. save both keys in .secret_key.txt in the same directory as app.py \n",
      "\n",
      "Original Title:\n",
      " maybe 2 key files?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cluck ci lags objective trunk services 20minutes 20minutes tar recommended characters canonical\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 39 =================\n",
      "\n",
      "\"https://github.com/log2timeline/plaso/issues/1528\"\n",
      "Issue Body:\n",
      " in some recent discussion threads make it more clear in the help indicate that --artifact_definitions should point to the directory containing the yaml files. \n",
      "\n",
      "Original Title:\n",
      " in tool help indicate that artifact_definitions should point to the directory containing the yaml files\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " kubelet jmod codeship profiler uncaught cambiar create sockets box broadly orange accordion\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 178 =================\n",
      "\n",
      "\"https://github.com/intersystems-ru/webterminal/issues/85\"\n",
      "Issue Body:\n",
      " this statement does not highlights properly in webterminal: while a > 2 { write 1 write 2 } \n",
      "\n",
      "Original Title:\n",
      " syntax highlight in while loop\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " purpose верна nest plexpy recompile polygon charaset host paper paper push action\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 21 =================\n",
      "\n",
      "\"https://github.com/dimsemenov/PhotoSwipe/issues/1359\"\n",
      "Issue Body:\n",
      " hello, first thanks for you amazing work. i'm using photoswipe to build a gallery for images contained in a blog post. first, i get all images, and build all the items objects with it, so i get an array like the following : js { src : http://mysite.local/uploads/img/681.jpeg?width=1280&height=819 , w : 1280, h : 819 }, // ... other items then i put a click event listener on every image, that gets the index of the image, and execute the following function : js show index { const options = { bgopacity: 0.85, closeonscroll: false, index, }; const gallery = new photoswipe this.galleryelement, photoswipeuidefault, this.items, options ; gallery.init ; } this.galleryelement is the pswp element given in the getting started http://photoswipe.com/documentation/getting-started.html , and this.items is the array containing all the items. but, when open is executed, the gallery pops but i can't do anything with it and i get the following error in the console : typeerror: item.initialposition is undefined . do you have an idea of what i'm doing wrong ? i can provide missing details if you need it. thank you very much. \n",
      "\n",
      "Original Title:\n",
      " typeerror: item.initialposition is undefined when initializing a gallery\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " second products fastled basilisk bigquery preview bare loading distorts push float global\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 171 =================\n",
      "\n",
      "\"https://github.com/Arasthel/Ghostly/issues/4\"\n",
      "Issue Body:\n",
      " got the disqus comments working swimmingly, but didn't find any comment count in the feed. is this perhaps something i will have to add myself? cheers! \n",
      "\n",
      "Original Title:\n",
      " comment count for disqus?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " second variation chronograph difficulty topics freenode pt neosnippet titlers neccessary mempoolfactorybuider air\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5 =================\n",
      "\n",
      "\"https://github.com/GoogleCloudPlatform/nodejs-docs-samples/issues/348\"\n",
      "Issue Body:\n",
      " daharon-macpro:speech daharon$ node recognize.js sync ./resources/audio.raw -e linear16 -r 16000 transcription: how old is the brooklyn bridge daharon-macpro:speech daharon$ node recognize.js async ./resources/audio.raw -e linear16 -r 16000 transcription: how old is the brooklyn bridge,, daharon-macpro:speech daharon$ node recognize.js async ./resources/audio.raw -e linear16 -r 16000 transcription: how old is the brooklyn bridge,, \n",
      "\n",
      "Original Title:\n",
      " async seems to create an additional ,, added to the transcript\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " broadcast sapeurs skal loading pages measure passionately passionately c department wording обратного\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36 =================\n",
      "\n",
      "\"https://github.com/j-medland/simion-gem-builder/issues/2\"\n",
      "Issue Body:\n",
      " add shorthand methods to add include and comment statements at any level. \n",
      "\n",
      "Original Title:\n",
      " add include and comment methods to appendable entity class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sequelize thumbnail signatures story enh triggered versus webdrivers issues звонка inform findpoas\n"
     ]
    }
   ],
   "source": [
    "# this method displays the predictions on random rows of the holdout set\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluate Model: BLEU Score\n",
    "\n",
    "For machine-translation tasks such as this one, it is common to measure the accuracy of results using the [BLEU Score](https://en.wikipedia.org/wiki/BLEU).  The convenience function illustrated below uses [NLTK's corpus_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.corpus_bleu).  The output of the below convenience function is an Average of BlEU-1, BLEU-2, BLEU-3 and BLEU-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n",
      "WARNING:root:Calculating BLEU.\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#convenience function that generates predictions on holdout set and calculates BLEU Score\n",
    "\n",
    "bleu_score = seq2seq_inf.evaluate_model(holdout_bodies=testdf.body.tolist(), \n",
    "                                        holdout_titles=testdf.issue_title.tolist(), \n",
    "                                        max_len_title=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score (avg of BLUE 1-4) on Holdout Set: 1.1214381572182756e-229\n"
     ]
    }
   ],
   "source": [
    "print(f'BLEU Score (avg of BLUE 1-4) on Holdout Set: {bleu_score * 100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
